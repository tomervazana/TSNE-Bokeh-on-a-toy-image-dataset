{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a631c412-f25d-4059-a1ce-418f8a4f9561",
   "metadata": {},
   "source": [
    "# Data Analysis using TSNE + Bokeh on toy image dataset\n",
    "\n",
    "#### The code purpose is:\n",
    "- Perform TSNE and display the source image, the label of the image, and the path of the source image (path need adjustment)\n",
    "- Embed the images using CLIP\n",
    "- Perform PCA if param value chosen is greater then 0\n",
    "- Provide a nice starting point to clean TSNE pipleline, just edit the configuration cells below to change dataset and parametes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f3d7c1-54d9-4510-8285-55c95d4bc36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file, save\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import Category10\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "\n",
    "def run_tsne_bokeh_visualization(\n",
    "    features: np.ndarray,\n",
    "    images: list,\n",
    "    labels: list,\n",
    "    image_paths: list = None,\n",
    "    image_size: tuple = None,\n",
    "    pca_n_components: int = 30,\n",
    "    tsne_params: dict = None,\n",
    "    figure_width: int = 900,\n",
    "    figure_height: int = 700,\n",
    "    output_path: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform PCA → t-SNE on a high-dimensional image dataset and render an interactive Bokeh plot\n",
    "    with hoverable thumbnails and labels. If `output_path` is provided, save as a standalone\n",
    "    HTML file; otherwise display inline in the notebook.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : np.ndarray\n",
    "        An (N × D) array of feature vectors (e.g., CNN embeddings or raw pixel vectors).\n",
    "    images : list of PIL.Image\n",
    "        A list of length N containing PIL Image objects corresponding to each feature.\n",
    "    labels : list or np.ndarray\n",
    "        A list/array of length N containing the (string-convertible) label for each image.\n",
    "    image_paths : list of str, optional\n",
    "        A list of length N containing the file path (or URL) for each image. If None, tooltips\n",
    "        will omit the path line.\n",
    "    image_size : tuple (width, height) or None, default=None\n",
    "        If None, each thumbnail uses its original PIL dimensions. Otherwise, resize each thumbnail\n",
    "        to (width, height) before encoding.\n",
    "    pca_n_components : int, default=30\n",
    "        Number of principal components to retain before feeding into t-SNE (to accelerate convergence\n",
    "        and reduce noise). If < 1, PCA is skipped and `features` are used directly.\n",
    "    tsne_params : dict, optional\n",
    "        Keyword args for sklearn.manifold.TSNE. If None, defaults to:\n",
    "            {\n",
    "                \"n_components\": 2,\n",
    "                \"perplexity\": 30,\n",
    "                \"learning_rate\": 200,\n",
    "                \"max_iter\": 1000,  # mean no FutureWarning\n",
    "                \"random_state\": 42,\n",
    "                \"init\": \"random\",\n",
    "            }\n",
    "    figure_width : int, default=900\n",
    "        Width of the Bokeh figure in pixels.\n",
    "    figure_height : int, default=700\n",
    "        Height of the Bokeh figure in pixels.\n",
    "    output_path : str, default=None\n",
    "        If provided, the function will save a standalone HTML to this path. Otherwise it\n",
    "        renders inline via output_notebook() and show().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Either displays an interactive Bokeh scatter plot in the notebook or writes an HTML file.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Validate inputs\n",
    "    n = features.shape[0]\n",
    "    if len(images) != n or len(labels) != n:\n",
    "        raise ValueError(\"`features`, `images`, and `labels` must all have the same length N.\")\n",
    "    if image_paths is not None and len(image_paths) != n:\n",
    "        raise ValueError(\"`image_paths` must be None or a list of length N.\")\n",
    "\n",
    "    # 2. Default t-SNE parameters if none provided\n",
    "    if tsne_params is None:\n",
    "        tsne_params = {\n",
    "            \"n_components\": 2,\n",
    "            \"perplexity\": 30,\n",
    "            \"learning_rate\": 200,\n",
    "            \"max_iter\": 1000,  # uses max_iter to avoid deprecation\n",
    "            \"random_state\": 42,\n",
    "            \"init\": \"random\",\n",
    "        }\n",
    "\n",
    "    # 3. Run PCA if requested (pca_n_components >= 1), else skip\n",
    "    if pca_n_components >= 1:\n",
    "        pca = PCA(n_components=min(pca_n_components, features.shape[1]))\n",
    "        features_reduced = pca.fit_transform(features)\n",
    "        print(\"PCA Done! Reduced shape:\", features_reduced.shape)\n",
    "    else:\n",
    "        features_reduced = features\n",
    "        print(\"PCA Skipped! Using original features shape:\", features_reduced.shape)\n",
    "\n",
    "    # 4. Run t-SNE\n",
    "    tsne = TSNE(**tsne_params)\n",
    "    embeddings = tsne.fit_transform(features_reduced)\n",
    "\n",
    "    # 5. Encode thumbnails as Base64\n",
    "    def pil_to_base64_with_size(img: Image.Image):\n",
    "        if image_size is None:\n",
    "            img_to_encode = img\n",
    "            w, h = img.size\n",
    "        else:\n",
    "            img_to_encode = img.resize(image_size, Image.NEAREST)\n",
    "            w, h = image_size\n",
    "        buffer = io.BytesIO()\n",
    "        img_to_encode.save(buffer, format=\"PNG\")\n",
    "        return \"data:image/png;base64,\" + base64.b64encode(buffer.getvalue()).decode(), w, h\n",
    "\n",
    "    thumbnails_b64, thumb_ws, thumb_hs = [], [], []\n",
    "    for img in images:\n",
    "        b64_str, w, h = pil_to_base64_with_size(img)\n",
    "        thumbnails_b64.append(b64_str)\n",
    "        thumb_ws.append(w)\n",
    "        thumb_hs.append(h)\n",
    "\n",
    "    # 6. Build ColumnDataSource\n",
    "    data_dict = {\n",
    "        \"x\": embeddings[:, 0],\n",
    "        \"y\": embeddings[:, 1],\n",
    "        \"thumb\": thumbnails_b64,\n",
    "        \"thumb_w\": thumb_ws,\n",
    "        \"thumb_h\": thumb_hs,\n",
    "        \"label\": [str(lbl) for lbl in labels],\n",
    "    }\n",
    "    if image_paths is not None:\n",
    "        data_dict[\"img_path\"] = image_paths\n",
    "    else:\n",
    "        data_dict[\"img_path\"] = [\"\"] * n\n",
    "\n",
    "    source = ColumnDataSource(data=data_dict)\n",
    "\n",
    "    # 7. Prepare the flexbox-based tooltip HTML\n",
    "    tooltip_html = \"\"\"\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "        <div style=\"flex: none; margin-right: 10px;\">\n",
    "            <img src=\"@thumb\" width=\"@thumb_w\" height=\"@thumb_h\" alt=\"@thumb\"/>\n",
    "        </div>\n",
    "        <div style=\"display: flex; flex-direction: column; justify-content: center;\">\n",
    "            <div style=\"font-size: 12px; font-weight: bold;\">@label</div>\n",
    "            <div style=\"font-size: 10px; color: #555; margin-top: 4px;\">@img_path</div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    hover_tool = HoverTool(tooltips=tooltip_html)\n",
    "\n",
    "    # 8. Color mapper for categorical labels\n",
    "    unique_labels = sorted(set(data_dict[\"label\"]))\n",
    "    color_mapper = factor_cmap(\n",
    "        field_name=\"label\",\n",
    "        palette=Category10[len(unique_labels)],\n",
    "        factors=unique_labels,\n",
    "    )\n",
    "\n",
    "    # 9. Create figure\n",
    "    p = figure(\n",
    "        title=\"t-SNE Embeddings\",\n",
    "        tools=[hover_tool, \"pan\", \"wheel_zoom\", \"reset\"],\n",
    "        x_axis_label=\"TSNE-1\",\n",
    "        y_axis_label=\"TSNE-2\",\n",
    "        width=figure_width,\n",
    "        height=figure_height,\n",
    "    )\n",
    "    p.scatter(\"x\", \"y\", size=10, source=source, color=color_mapper, alpha=0.8)\n",
    "\n",
    "    # 10. Render or save\n",
    "    if output_path:\n",
    "        # Ensure parent directory exists\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        output_file(output_path)\n",
    "        save(p)\n",
    "        print(f\"✅ Bokeh figure saved to: {output_path}\")\n",
    "    else:\n",
    "        output_notebook()\n",
    "        show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3acd86-c312-430a-9872-3f9cea348d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def convert_array_to_pil_images(image_array: np.ndarray, rescale: bool = True):\n",
    "    \"\"\"\n",
    "    Convert a NumPy array of shape (N, H, W) into a list of PIL.Image objects.\n",
    "\n",
    "    Args:\n",
    "        image_array (np.ndarray): \n",
    "            The source array with shape (N, H, W). \n",
    "            Expected dtype is float-like; values probably in [0, max_val].\n",
    "        rescale (bool): \n",
    "            If True, linearly scale each image from its [min, max] to [0, 255].\n",
    "            If False, assumes input is already in [0, 255] or you accept truncation.\n",
    "\n",
    "    Returns:\n",
    "        List[Image.Image]: A list of PIL Image objects (mode 'L' for grayscale).\n",
    "    \"\"\"\n",
    "    pil_images = []\n",
    "    # Determine per-array scaling if needed\n",
    "    for idx in range(image_array.shape[0]):\n",
    "        single = image_array[idx]  # shape (H, W)\n",
    "        \n",
    "        if rescale:\n",
    "            # Compute min/max per image to avoid saturating outliers\n",
    "            min_val = single.min()\n",
    "            max_val = single.max()\n",
    "            # Avoid division by zero if the image is constant\n",
    "            if max_val > min_val:\n",
    "                scaled = (single - min_val) / (max_val - min_val) * 255.0\n",
    "            else:\n",
    "                # If max == min, the image is flat; produce a zeroed image\n",
    "                scaled = np.zeros_like(single)\n",
    "        else:\n",
    "            scaled = single\n",
    "        \n",
    "        # Cast to uint8; values outside [0, 255] will wrap or clip\n",
    "        array_uint8 = scaled.astype(np.uint8)\n",
    "        # Create a PIL Image in 'L' mode (8-bit grayscale)\n",
    "        img = Image.fromarray(array_uint8, mode='L')\n",
    "        pil_images.append(img)\n",
    "    \n",
    "    return pil_images\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Suppose `batch` is your array with shape (N, H, W), e.g. (6, 28, 28)\n",
    "# batch = np.array([...], dtype=float)\n",
    "\n",
    "# pil_list = convert_array_to_pil_images(numpy_array_of_images, rescale=False)\n",
    "# pil_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa949bb-23e4-43e5-8dff-4d56f0e949a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configurations:\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits() # load toy image dataset\n",
    "\n",
    "# More Dataset Configurations:\n",
    "numpy_array_of_images = digits.images\n",
    "pil_list = convert_array_to_pil_images(numpy_array_of_images, rescale=False)\n",
    "# Now pil_list is a standard Python list of PIL.Image objects\n",
    "labels = digits.target.astype(str)\n",
    "images = [Image.fromarray((im * 16).astype(np.uint8)) for im in digits.images]\n",
    "# Adjust image_paths to get the real paths\n",
    "image_paths = [f\"/path/to/digits/digit_{i}.png\" for i in range(len(images))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79c2b1e4-7bd0-461b-989e-16cfb09a8031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "-----------------------------\n",
      "Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "-----------------------------\n",
      "Compose(\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _convert_image_to_rgb at 0x70655d0b49a0>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n",
      "-----------------------------\n",
      "\n",
      " array_of_embeddings shape is: (1797, 512)\n"
     ]
    }
   ],
   "source": [
    "# Create the features\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "print(device, model.visual.conv1, preprocess, sep='\\n-----------------------------\\n')\n",
    "# Suppose CLIP model is loaded already.\n",
    "\n",
    "# torch_of_embeddings = torch.empty((numpy_array_of_pil.shape[0], 512), dtype=torch.float16) #### numpy_array_of_pil ref to numpy_array_of_images in this code\n",
    "tensor_of_embeddings = torch.empty((len(pil_list), 512), dtype=torch.float16)\n",
    "for i in range(len(pil_list)): \n",
    "    image = pil_list[i]\n",
    "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "    # Calculate features\n",
    "    with torch.no_grad():\n",
    "        # len of numpy_array_of_pil must be eq to numpy_array_of_embeddings len\n",
    "        image_embeddings = model.encode_image(image_input)\n",
    "        tensor_of_embeddings[i] = image_embeddings\n",
    "\n",
    "array_of_embeddings = tensor_of_embeddings.cpu().numpy()\n",
    "\n",
    "print(f'-----------------------------\\n\\n array_of_embeddings shape is: {array_of_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767c5b74-f090-4e6b-b05e-3faa913be550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Skipped! Using original features shape: (1797, 512)\n",
      "✅ Bokeh figure saved to: /home/tomer-v-u/PycharmProjects/t-sne-test/tsne_bokeh_toy_output.html\n"
     ]
    }
   ],
   "source": [
    "# TSNE + Bokeh configurations\n",
    "\n",
    "tsne_params = {\n",
    "    \"n_components\": 2,\n",
    "    \"perplexity\": 50,\n",
    "    \"learning_rate\": 150,\n",
    "    \"max_iter\": 2000,\n",
    "    \"random_state\": 123,\n",
    "    \"init\": \"random\"\n",
    "}\n",
    "# features = digits.data # to use the pixel values as features\n",
    "features = array_of_embeddings # to use CLIP embeddings as features\n",
    "\n",
    "# Define output path (Optional)\n",
    "output_fn = 'tsne_bokeh_toy_output.html'\n",
    "output_path = os.path.join(os.getcwd(), output_fn)\n",
    "\n",
    "# Run the core function\n",
    "run_tsne_bokeh_visualization(\n",
    "    features, images, labels, image_paths,\n",
    "    image_size=(64, 64),\n",
    "    pca_n_components=-1,\n",
    "    tsne_params=tsne_params,\n",
    "    figure_width=800, figure_height=600,\n",
    "    output_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d8d0d-44bc-4a9f-a5b8-61bde24e81b4",
   "metadata": {},
   "source": [
    "#### Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2edba080-1788-43cc-8f3d-1c5f5b1c1b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "anyio                     4.9.0\n",
      "anywidget                 0.9.18\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.5\n",
      "attrs                     25.3.0\n",
      "babel                     2.17.0\n",
      "beautifulsoup4            4.13.4\n",
      "bleach                    6.2.0\n",
      "blinker                   1.9.0\n",
      "bokeh                     3.7.3\n",
      "certifi                   2025.4.26\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.2\n",
      "click                     8.2.1\n",
      "clip                      1.0\n",
      "comm                      0.2.2\n",
      "contourpy                 1.3.2\n",
      "cycler                    0.12.1\n",
      "dash                      3.0.4\n",
      "debugpy                   1.8.14\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "executing                 2.2.0\n",
      "fastjsonschema            2.21.1\n",
      "filelock                  3.13.1\n",
      "filetype                  1.2.0\n",
      "Flask                     3.0.3\n",
      "fonttools                 4.58.1\n",
      "fqdn                      1.5.1\n",
      "fsspec                    2024.6.1\n",
      "ftfy                      6.3.1\n",
      "h11                       0.16.0\n",
      "hdbscan                   0.8.40\n",
      "hf-xet                    1.1.2\n",
      "httpcore                  1.0.9\n",
      "httpx                     0.28.1\n",
      "huggingface-hub           0.32.2\n",
      "idna                      3.7\n",
      "importlib_metadata        8.7.0\n",
      "ipykernel                 6.29.5\n",
      "ipython                   9.2.0\n",
      "ipython_pygments_lexers   1.1.1\n",
      "ipywidgets                8.1.7\n",
      "isoduration               20.11.0\n",
      "itsdangerous              2.2.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.4\n",
      "joblib                    1.5.1\n",
      "json5                     0.12.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.24.0\n",
      "jsonschema-specifications 2025.4.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter_core              5.8.1\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.16.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.4.3\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "jupyterlab_widgets        3.0.15\n",
      "kiwisolver                1.4.8\n",
      "llvmlite                  0.44.0\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib                3.10.3\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.1.3\n",
      "mpmath                    1.3.0\n",
      "narwhals                  1.41.0\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.3\n",
      "notebook                  7.4.3\n",
      "notebook_shim             0.2.4\n",
      "numba                     0.61.2\n",
      "numpy                     2.1.2\n",
      "nvidia-cublas-cu12        12.6.4.1\n",
      "nvidia-cuda-cupti-cu12    12.6.80\n",
      "nvidia-cuda-nvrtc-cu12    12.6.77\n",
      "nvidia-cuda-runtime-cu12  12.6.77\n",
      "nvidia-cudnn-cu12         9.5.1.17\n",
      "nvidia-cufft-cu12         11.3.0.4\n",
      "nvidia-curand-cu12        10.3.7.77\n",
      "nvidia-cusolver-cu12      11.7.1.2\n",
      "nvidia-cusparse-cu12      12.5.4.2\n",
      "nvidia-cusparselt-cu12    0.6.3\n",
      "nvidia-nccl-cu12          2.21.5\n",
      "nvidia-nvjitlink-cu12     12.6.85\n",
      "nvidia-nvtx-cu12          12.6.77\n",
      "opencv-python             4.11.0.86\n",
      "opencv-python-headless    4.10.0.84\n",
      "overrides                 7.7.0\n",
      "packaging                 25.0\n",
      "pandas                    2.2.3\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pillow                    11.0.0\n",
      "pillow_heif               0.22.0\n",
      "pip                       25.1.1\n",
      "platformdirs              4.3.8\n",
      "plotly                    6.1.2\n",
      "prometheus_client         0.22.0\n",
      "prompt_toolkit            3.0.51\n",
      "psutil                    7.0.0\n",
      "psygnal                   0.13.0\n",
      "ptyprocess                0.7.0\n",
      "pure_eval                 0.2.3\n",
      "pycparser                 2.22\n",
      "Pygments                  2.19.1\n",
      "pynndescent               0.5.13\n",
      "pyparsing                 3.2.3\n",
      "python-dateutil           2.9.0.post0\n",
      "python-dotenv             1.1.0\n",
      "python-json-logger        3.3.0\n",
      "pytz                      2025.2\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.4.0\n",
      "referencing               0.36.2\n",
      "regex                     2024.11.6\n",
      "requests                  2.32.3\n",
      "requests-toolbelt         1.0.0\n",
      "retrying                  1.3.4\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "roboflow                  1.1.64\n",
      "rpds-py                   0.25.1\n",
      "safetensors               0.5.3\n",
      "scikit-learn              1.6.1\n",
      "scipy                     1.15.3\n",
      "Send2Trash                1.8.3\n",
      "setuptools                70.2.0\n",
      "six                       1.17.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.7\n",
      "stack-data                0.6.3\n",
      "supervision               0.25.1\n",
      "sympy                     1.13.1\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.6.0\n",
      "tinycss2                  1.4.0\n",
      "tokenizers                0.21.1\n",
      "torch                     2.6.0+cu126\n",
      "torchaudio                2.6.0+cu126\n",
      "torchvision               0.21.0+cu126\n",
      "tornado                   6.5.1\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "transformers              4.52.3\n",
      "triton                    3.2.0\n",
      "types-python-dateutil     2.9.0.20250516\n",
      "typing_extensions         4.12.2\n",
      "tzdata                    2025.2\n",
      "umap-learn                0.5.7\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.4.0\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "Werkzeug                  3.0.6\n",
      "widgetsnbextension        4.0.14\n",
      "xyzservices               2025.4.0\n",
      "zipp                      3.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec6efe09-ab0e-488a-88b0-b40dd704fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e64c26-9807-4598-9e23-9f5aa95b8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.6 (cu126)",
   "language": "python",
   "name": "torch26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
